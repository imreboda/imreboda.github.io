{
    "collab_server" : "",
    "contents" : "---\ntitle: \"CEU Unstructured Text Analysis Seminar - Assignment\"\nauthor: \"Imre Boda\"\ndate: '2018-03-30T21:13:14-05:00'\noutput:\n  html_document: default\n  pdf_document: default\ntags:\n- R Markdown\n- plot\n- regression\ncategories: R\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(eval = FALSE)   #not to run any chunk in this rmd file\n```\n\n# Introduction\n\nThis is my term assignment of the Unstructured Text Analysis Seminar at CEU in the 2018 winter semester.\nThe course was held by Mr. Eduardo Arino de la Rubia.\nThe assignment was about to create something that use the stuff taught and learnt in the course.\nI used a lot from the techniques described in the great book \"Text Mining with R\" by Julia Silge and David Robinson.\n\nMy work is a Facebook post and comment analyzer shiny dashboard. It looks at the Facebook pages of a few telco and related vendors (to some extent related to telcos' traditional or new business segments), and presents / compares their key marketing themes during the past few quarters. Also, looks at the comments to the posts and does a sentiment analysis on the them along the time. At the time of writing this blog, the program is hosted at AWS, available at http://boda.ceudata.net:3838/.\n\nThe dashboard consists of 3 main parts:\n\n- Presenting the key post themes of the selected vendor per quarter,\n- Compares the key Facebook themes of the selected vendors to a benchmark vendor,\n- Shows the sentiment scores of the two vendors along time.\n\nKey Facebook \"themes\" in this context are the most often used phrases (words and bi-grams). When I played with more sophisticated algorithms (tf-idf, LDA), they showed no better result. Maybe this is due to the nature of these posts, so finally I stayed with this simple way.\n\nSentiment scoring is based on summing up the words with positive / negative sentiment load, correcting it with a bi-gram sentiment check (making sure that terms such as \"not nice\" are properly calculated as negative not as positive).\n\n![Dashboard screenshot.](/img/Dashboard.png)\n\n\n\nThe main steps of the works were the followings:\n\n- Setting up the infrastructure on AWS,\n- Obtaining Facebook token,\n- Writing a script that pulls, stores and analyses posts & comments from Facebook,\n- Scheduling daily execution in Jenkins,\n- Creating the shiny app.\n\n\n\n#Setting up the infrastructure on AWS\n\nIt consisted of the following steps:\n\n-\tCreating EC2 instance with Ubuntu,\n- In the \"Security Group\" settings opening ports for Rstudio, Jenkins and Shiny,\n-\tInstalling RStudio, creating RStudio admin user on Ubuntu \n-\tInstalling shiny and shiny server,\n- Installing R packages from the command line in order to make them available for all R users (needed for script scheduled from Jenkins),\n-\tInstalling Jenkins, customizing, creating Jenkins admin user.\n\n\n\n#Obtaining a Facebook token\n\nFairly short, but for a Facebook analysis obviously inevitable workflow:\n\n- Registering at Facebook Developer page (https://developers.Facebook.com),\n- Creating a new App, getting the App ID and App Secret,\n- On Facebook App login setting site URL of the App to \"http://localhost:1410/\",\n- From Rstudio (with Rfacebook installed), starting fboAuth with the App ID and App Secret parameters obtained in the previous step,\n- Following the instructions seen on the console,\n- Saving the received token somewhere for later use.\n\n\n\n#The script that pulls / stores post and comment data from Facebook\n\nThis script is run daily, as scheduled from Jenkins. It fetches and stores Facebook posts and comments of selected vendors. It every night scrapes all such data for a given period, regardless that all except last day's info have been collected before. I thought that disregarding earlier collected data and recollecting again is a cleaner (though not economic) way then relaying on data gathered and stored before, checking if that data is fully valid and not corrupted in any way, no new comments received to earlier posts, etc. \nObtaining (dominantly the same) data every day is more work for machines, but this is why we have machines. It is cleaner, and after all, the data volume is not super high, so it causes no disturbance to anyone.\n\n\n### Fetching, a bit cleaning and transforming data from Facebook\n\nFirst, it loads the Facebook Authentication token (saved as per the previous chapter).\n\nThen in the main section from a given date (At the time of writing it is \"2017-01-01\". I don't think I will leave this running for long, but if I will, then it will be modified to be 5 - 7 quarters back from the actual date) \nuntil the day before, in a cycle it does the following:\n\n- For each vendor's page it reads the posts in 5 days chunks, as the number of posts returned in one query has a limit by Facebook. Transforms the date info a bit, as some display will be done on a per quarter basis, and stores the result in a \"posts table\".\n- For each collected post, it reads all comments and stores them in another table, the \"comments table\".\n- Since many posts do not have message field, but they point to an article link with meaningful URL (e.g. \"http://www.adweek.com/tv-video/how-watson-is-digging-through-decades-of-video-to-help-find-new-sources-of-revenue/\"), the script extracts meaningful info from the URL for such posts with empty message field. It searches for patterns that starts with \"/\", followed by at least 3 times \"one or more word characters followed by -\" then again some word characters (such pattern is \"how-watson-is-digging-through-decades-of-video-to-help-find-new-sources-of-revenue/\"). The reason for requesting at least 3 times of \"word characters plus '-'\" sequences and then some word characters is to exclude patterns like \"/2017-03-01/\", pretty typical in URLs. Such patterns are cleaned from leading \"/\" and separator \"-\" characters and copied to a new field called \"Headline\", to make up for the empty \"message\" field. In vast majority of the cases this seems to do the job.\n```{r}\n### extracting headline from news link, if message is empty\nVendorPosts_womessage <- VendorPosts%>%\n    filter(is.na(message)) %>%\n    mutate (Headline = str_extract (link, \"/(\\\\w+-){3,}[\\\\w]+\")) %>%    #search for patterns like \"/5g-connected-trials-c..\"\n    mutate (Headline = str_replace (Headline, \"^/\",\"\")) %>%   #strip leading / \n    mutate (Headline = str_replace_all (Headline, \"-\", \" \")) \n```\n\n- For the rest of the posts, which do contain \"message\" info, this field is copied to the newly created \"Headline\" column, getting rid of potential \":http...\" references, which often appended to the end of Facebook posts in the  form of \"read more at: http...\", \"learn more: http...\" and alike.\n```{r}\n### copy message to headline for the rest\n  VendorPosts_wmessage <- VendorPosts %>% \n    filter(!is.na(message)) %>%\n    mutate (Headline = message) %>%\n    mutate (Headline = str_replace (Headline, \"htt.*$\", \"\")) %>%    #remove http.....\n    mutate (Headline = str_replace (Headline, \":\\\\s*$\", \"\"))        #remove \": \", mostly inherited from \": http...\"\n```\n\nThis way all real or created messages are placed in column \"Headline\", (most often) containing the story.\n\n- As mentioned above, messages often contain \"learn more\", \"can find\",... phrases, which are in this context to be treated as junks, so these are filtered out.\n``` {r}\n  VendorPosts <- VendorPosts %>%\n    mutate (Headline = str_replace_all( Headline, \"read more\", \"\")) %>%\n    mutate (Headline = str_replace_all (Headline, \"learn more\", \"\")) %>%\n    mutate (Headline = str_replace_all (Headline, \"can find\", \"\"))\n```\n\n- Finally, a mega table creating all posts and relevant comments are created with joining \"posts table\" and \"comments table\". The above are repeated for all vendors, and the result (posts and comments) are stored in \"AllPostsComments\".\n```{r}\nVendorPosts <- VendorPosts %>% left_join(VendorCommentsTable, by = c(\"id\"=\"PostID\"))\n\n  ### Putting to one list  \nAllPostsComments <- rbind (AllPostsComments, VendorPosts)\n```\n\nThe final table is saved in a file, just in any case. \n\n\n##Fetching, a bit cleaning and transforming data from Facebook\n\nOnce all above are done, comes the shallow text mining part, which consists of two parts:\n\n- Finding the key post themes,\n- Sentiment analysis on the comments.\n\n\n####Finding key themes in posts\n\nAs said above, this is based on simply getting the most frequent words and bi-grams in the posts. \nAs mentioned in the introduction section, I played with tf-idf and LDA, but their output was not better than the simple term frequency, so that was selected for the finally implemented version.\n\nAs themes are presented and compared on a per vendor and per quarter basis, 5 most frequent words and 5 most frequent bi-grams are to be marked per quarter per vendor. It is done in two rounds, first for uni-grams and then for bi-grams. The reason for doing it in two separate rounds is that naturally word frequencies are higher than bi-gram frequencies, therefor if top phrases would be collected in one go, only words could qualify for the top 5 or top 10 places, not bi-grams.\n\n\nFirst we get rid of duplicate posts (introduced when we merged \"posts table\" and \"comments table\", as one post had typically several comments), word tokenize the \"Headline\" column, get rid of normal stopwords and special stopwords (containing the vendors' names and some words that they frequently use in posts so here become junk).  \n\nThen the words' frequency per vendor per quarter is added to each remaining word.\n\n```{r}\ntidyPosts <- AllPostsComments %>%\n  group_by(id) %>%                #from here until ungroup: get rid of duplicate posts\n  filter(row_number() == 1) %>%\n  ungroup %>%\n  unnest_tokens (word, Headline) %>%\n  filter(!is.na(word)) %>%\n  anti_join (stp_en) %>%\n  anti_join (stp_spc) %>%\n  mutate(word = str_replace(word, \"\\\\w+[^\\\\w]\\\\w*\", \"\")) %>%   #\"zte's\" was not propoerly filtered by stp_spc\n  filter(word != \"\") \n\nproportion <- tidyPosts %>%\n  count (Vendor, created_Q, word, sort = TRUE) %>%   #count words per vendor per Q\n  group_by (Vendor, created_Q) %>%\n  mutate (prop =  n/sum(n) * 100) %>%\n  arrange (desc(prop)) %>%\n  arrange (Vendor, created_Q)\n```\n\n\nBi-grams are treated pretty similarly, except that for getting rid of stopwords, \"specific\" stopwords and \"zte's\" like words, bi-grams are temporarily separated, and after the stopwords removal united back.\n\n```{r}\nbigramPosts <- AllPostsComments %>%\n  group_by(id) %>%                #from here until ungroup: get rid of duplicate posts\n  filter(row_number() == 1) %>%\n  ungroup %>%\n  unnest_tokens (bigram, Headline, token = \"ngrams\", n = 2) %>%\n  filter(!is.na(bigram)) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%   #storing bigram words separately\n  filter(!word1 %in% stp_en$word) %>%\n  filter(!word2 %in% stp_en$word) %>%\n  filter(!word1 %in% stp_spc$word) %>%\n  filter(!word2 %in% stp_spc$word) %>%\n  mutate(word1 = str_replace(word1, \"\\\\w+[^\\\\w]\\\\w*\", \"\")) %>%   #\"zte's\" was not propoerly filtered by stp_spc\n  filter(word1 != \"\")  %>%\n  mutate(word2 = str_replace(word2, \"\\\\w+[^\\\\w]\\\\w*\", \"\")) %>%   #\"zte's\" was not propoerly filtered by stp_spc\n  filter(word2 != \"\")  %>%\n  unite(word, word1, word2, sep = \" \") \n\nproportion_bi <- bigramPosts %>%   #same as for unigrams, not collapsed as these have different weights\n  count (Vendor, created_Q, word, sort = TRUE) %>%   #count words per vendor per month\n  group_by (Vendor, created_Q) %>%\n  mutate (prop =  n/sum(n) * 100) %>%\n  arrange (desc(prop)) %>%\n  arrange (Vendor, created_Q) \n```\n\nFinishing the key phrase selection parts, the top 5 words and bi-grams (per vendor per quarter) are put in one list and saved. The same way the lists of all (valid) words and bi-grams with their frequency proportion are saved.\n\n```{r}\nprop_top5 <- rbind (\n  proportion  %>%\n    filter (row_number () <6L),\n  proportion_bi  %>%\n    filter (row_number () <6L)\n)\nfwrite (prop_top5, paste0(DIR, \"/\", \"prop_top5.csv\"))\n\n\nproportion <- rbind (proportion, proportion_bi)\nfwrite (proportion, paste0(DIR, \"/\", \"proportion.csv\"))\n```\n\nThese files are used to pass information between Facebook input collector and shiny display output functions. For a while I went for an alternative approach: to use redis as a postbox. I ended up with the file based version as files are useful anyhow to cope with shutdown and similar situations. And as such reads do not take place frequently (at least as long as the dashboard does not attract millions of visitors :) ), it should be fine.\n\n\n####Comments sentiment analysis\n\nConcluding the text mining parts, the sentiments of comments to vendors' post are analysed based on the \"bing\" sentiment lexicon. The idea is that words with positive and negative sentiments are counted for every day for every vendor, and the difference of the two will be the sentiment score of the vendor for that day.\n\nThe above idea is slightly enhanced here, as bi-grams are also looked at, and if there is a negating word (e.g \"not\", \"isn't\", etc.) preceding an adjective word, then the sentiment classification of the word is negated. \n\nAgree, this is still not \"bullet proof\" as there are negations that are still not treated (and not only sarcastic sentences or complex structures, but simple cases such as \"not quite good\", not to mention comments written in non-English language), but probably it is a safe assumption that these cases are just minority, and the result is \"acceptably good enough\".\n\nSo as before, the analysis is run in two steps.\n\nFirst, words with sentiment load are counted per vendor and per day.\n\n```{r}\nbing <- (get_sentiments( \"bing\"))\n\n\ntidy_word_Comments <- AllPostsComments %>%\n  unnest_tokens (word, Comment_message) %>%\n  filter(!is.na(word)) %>%\n  filter(word != \"\") \n\nuni_res <- tidy_word_Comments %>%\n  inner_join (bing) %>%\n  count (Vendor, as.Date(Comment_created_time), sentiment) %>%     #Comment_created_wk for weekly display version[]\n  spread (sentiment, n, fill = 0) %>%\n  mutate (sentiment = positive - negative) \n```\n\nThen for correcting the cases when an adjective is preceded with a negation word, first such cases are filtered. Then the sentiment score of the bi-gram based on its **second word** is counted for every day (and every vendor), and then negated.\n\nFinally, the original sentiment scores for every day are corrected with the above \"delta sentiment\" scores by joining the result of the two calculations and subtracting twice the correction scores (the twice has to be calculated, as e.g. for \"not good\" in the uni-gram score we increased the sentiment, adding the negated score once would only take it to neutral, another correction is needed to get the correct score).\n\nAs always, the result is written to a file to pass it to the dashboard application.\n\n```{r}\n## bi-grams to take care of negations\nnegation_words <- c(\"not\", \"no\", \"never\", \"without\", \"isn't\", \"aren't\")\n\n\ntidy_bigram_Comments <- AllPostsComments %>%\n  unnest_tokens (bigram, Comment_message, token = \"ngrams\", n = 2) %>%\n  filter(!is.na(bigram)) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")    #storing bigram words separately\n\nnegated_words_res <- tidy_bigram_Comments %>%\n  filter(word1 %in% negation_words) %>%\n  inner_join(bing, by = c(\"word2\" = \"word\")) %>%\n  count (Vendor, as.Date(Comment_created_time), sentiment) %>%   \n  mutate(sentiment = recode (sentiment, \"positive\" = \"negative\", \"negative\" = \"positive\")) %>%    #invert\n  spread (sentiment, n, fill = 0) %>%\n  mutate (delta_sentiment = positive - negative) %>%\n  select (-c(negative, positive))\n\n#joining the result of bigram and calculate sentiment\ncomment_sent <- left_join(uni_res, negated_words_res) %>%\n  mutate (delta_sentiment = ifelse(is.na(delta_sentiment),0,delta_sentiment)) %>%\n  mutate (sentiment = sentiment + 2 * delta_sentiment)\n```\n\n\n\n#Scheduling the Facebook post and comment collector and analyzer\n\nThe above script is schedule in Jenkins to run daily as shown in the below screenshot.\n\n![Jenkins scheduler setting](/img/JenkinsSchedule.png)\n\n\n\n#The shiny dashboard app\n\n\nThe standard shiny Dashboard layout is used, which consists of a header, a sidebar and a main body part.\n\nThe sidebar contains nothing else but a selectbox to choose the vendor to be analyzed.\n\nThe body section consists of three parts:\n\n- The upper part displays the key themes of the selected vendor by quarters. The most often used 5 \"real\" words and bi-grams are regarded as key topics, as described above.\n- The middle layer allows a quarter and a second vendor selection via two other selectboxes. The idea is to allow some comparison to reveal how similar or different two telco vendors' key themes are; to see if there are any significant differences between their slogans that might reveal different market strategy. So under the selectboxes the two vendors' key themes in the selected quarter are plotted (in wordcloud), and also a graph that displays the frequencies of the themes (words and bi-grams) used by both vendors. This graph reveals how many common themes these have in their posts (the more words presented on the plot), and also how much emphasis the vendors give to these themes (the closer the points are to the diagonal line, the more similar emphasis is given to a theme represented by the word/bi-gram),\n- The bottom part shows the sentiment score of the comments given to the vendors' posts by time.\n\nThe dashboard is written in a single app file, containing both the ui and the server functions.\n\n\n\n####The input selectors\n\nThere are three selectbox inputs.\n\nThe first one allows to chose the \"main\" vendor. This is looked at in all three dashboard sections.\n\nIts selectbox is placed in the sidebar as shown below:\n\n```{r}\nsidebar <- dashboardSidebar(\n  \n  selectInput(inputId = \"vendorselect\",\n              label = \"Select Vendor\",\n              choices = Vendor_list)\n  )\n```\nVendor_list is read at the server start (in case of change in vendors, it has to be restarted).\n\n\nThe second selector allows a \"comparison\" vendor to select, this is the one whose market themes are compared to the \"main\" vendor's in the middle section, and the comments' sentiments also in the bottom section.\n\nGiving value to this is slightly more work than doing for the \"main\" vendor. \n\nFirst, because here we need to make sure not to select the \"main\" vendor; the latter must be excluded from the list of selectable vendors.\n\nSecond, because simply excluding the \"main\" vendor from the selectable list via \"choices = Vendor_list [Vendor_list != input$vendorselect]\" results in an error message displayed at the middle section's middle plot for about half a second. After that it disappears, so not a big deal indeed, but still not nice.\nIn order to get rid of this disturbing error, a reactive variable is introduced, which is first initialized with the second item on the vendors' list. When the user selects a second vendor, this reactive variable is getting new value. All plotting functions presenting anything about second vendor will use this reactive variable, and not directly the output if the input selector for the second vendor. This way these plotting functions will always get non-NULL value.\n\n```{r}\n  vendor2 <- reactiveVal(Vendor_list [2]) \n  \n  output$select_vendor2 <- renderUI({\n    selectInput(inputId = \"vendor2\",\n                label = \"Select Another Vendor to compare\",\n                choices = Vendor_list [Vendor_list != input$vendorselect])\n  })\n  \n  observeEvent(input$vendor2, {\n    vendor2(input$vendor2)             # rv$value <- newValue\n  })\n```\n\n\n\nThe third input selector allows picking the quarter for the theme comparison in the middle section. It is a simple selectInput, the same as in the sidebar, except that here the list to be selected from is the list of quarters of the analysis period.\n\n\n####The output functions: plots\n\n\n#####Top section\n\nIt is a simple \"jitter plot\" with the relative frequency of the word / bi-gram on the y, and quarter on the x axis. The input is the top5 (top5 per word plus top5 per bi-gram, per quarter per vendor) results of the post and comment reader and processor script.\n\n```{r}\noutput$plot1 <- renderPlot({prop_top5 %>% filter (Vendor == input$vendorselect)%>%\n      ggplot (aes(created_Q, prop, color=created_Q, label = word)) + geom_jitter (size =0) + \n      geom_text_repel (size=5, segment.color = \"white\") + theme_bw (base_size = 24) + \n      theme(legend.position=\"none\", panel.grid.major=element_blank(),\n            panel.grid.minor=element_blank(), axis.text.y=element_blank(), axis.line = element_blank()) +\n      labs(title = sprintf (\"Most frequent themes in %s Facebook posts\", toupper(input$vendorselect))) +\n           xlab(\"Quarters\") + ylab  (\"Frequency\")\n    })\n```\n\n\n\n#####Middle section\n\nThe two wordclouds at the two ends work pretty much the same way, except that the one for the \"comparison\" vendor uses the reactive variable, while the other one can take the result of its inputSelect directly. They both work on the same data as the plot in the top section.\n\n```{r}\n  output$cloud1 <- renderPlot({\n    prop_top5 %>% filter (Vendor == input$vendorselect) %>% filter (created_Q == input$quarterselect) %>%\n      with(wordcloud(word, n, max.words = 100, min.freq = 0.2, scale = c(3,0.5), colors = c(\"#0091ff\", \"#f0650e\")))  \n  })\n  output$cloud2 <- renderPlot({\n    prop_top5 %>% filter (Vendor == vendor2()) %>% filter (created_Q == input$quarterselect) %>% \n      with(wordcloud(word, n, max.words = 100, min.freq = 0.2, scale = c(3,0.5),colors = c(\"#0091ff\", \"#f0650e\")))  \n  })\n```\n\n\nThe middle plot looks at the frequency table of all uni-grams and bi-grams and plots it on a \"jitter plot\". For every word coming to the plot the relative frequency in the \"comparison\" vendor's frequency table is presented on the x axis, while on the y axis the same for the \"main\" vendor. \n\nThe scale is logarithmic, consequently, words and bi-grams that are present in one but not in the other's posts are not even shown. This means that the less points are on the displays, the less common themes the vendors have.\nAnd as said above, the closer is a point to the diagonal line, it has more similar frequency in the vendors' posts. In an oversimple interpretation: getting the same focus.\n\n```{r}\n  output$CompPlot <- renderPlot ({\n    proportion %>%\n      filter (created_Q == input$quarterselect) %>%\n      filter (Vendor %in% c(input$vendorselect, vendor2())) %>%\n      select (-n) %>%     #drop n, in order  to allow spread and collapse (gather)\n      spread (Vendor, prop) %>%    #spred prop per vendor\n      gather (Vendor, prop, -created_Q, -word, -c(input$vendorselect, vendor2())) %>%\n      ggplot ( aes(get(vendor2())/100, get(input$vendorselect)/100, \n                   color = abs (get(input$vendorselect)-get(vendor2())))) + \n      geom_abline (color = \"gray40\", lty =2) + \n      geom_jitter (alpha = 0.3, size = 2.5, width = 0.3, height = 0.3) +\n      geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n      scale_x_log10(labels = percent_format(), limits = c(0.001,0.1)) +\n      scale_y_log10(labels = percent_format(), limits = c(0.001,0.1)) +\n      scale_color_gradient(limits = c(0, 2), low = \"#0091ff\", high = \"#f0650e\") +\n      #facet_wrap (~ Vendor, ncol = 5) +\n      theme_bw() + theme(legend.position=\"none\") +\n      labs(y = input$vendorselect, x = vendor2())\n      \n  })\n```\n\nAt the beginning I planned to represent such similarity and dissimilarity with commonality and comparison clouds from the same \"wordcloud\" package. However, it seemed not to be trivial how to cope with cases when a message was in high focus at both vendors, but with different weights. (e.g. this was the case with Ericsson and ZTE: both had 5G as their #1 topic by far, but as in Ericsson's posts it appeared with even higher frequency than in ZTE's, it was shown as a difference between the two. Whereas the reality is that it was by far the most exposed topic at both vendors, hence, it is a strong similarity in their strategic focus, not dissimilarity).\n\nI am sure that there is some nice way to overcome this, but for the sake of time I decided to leave the cloud comparison for \"inside\" vendors' topics and use this (visually less attractive) solution: cloud where font sizes represent frequency among the given vendor's topics (not across vendors).\n\n\n#####The bottom section\n\nFinally, there come two highcharts to present the comments sentiment scores for the two selected vendors: the upper one displays the \"main\" vendor's by time, the lower one the \"comparison\" vendor's. I picked the \"stock\" type highchart for such display, because this creates amazing timeline charts.\n\nThe two outputs are pretty much alike, the only notable difference is that the second one uses here again the reactive variable, while the main vendors' chart takes value from the relevant input selector directly.\n\n\n```{r}\noutput$hc <- renderHighchart({\n    hc <- comment_sent %>% \n      filter (Vendor == input$vendorselect) %>% \n      hchart(\"line\", hcaes(x=as.Date(`as.Date(Comment_created_time)`), y = sentiment)) %>%        \n      hc_xAxis(type = \"datetime\") %>%\n      hc_title (text = sprintf(\"Comments' sentiments of %s\", toupper(input$vendorselect)))\n    hc <- hc_tooltip(hc, pointFormat = \"Daily Sentiment Score: {point.y}\")\n      \n    hc$x$type <- \"stock\"                              #\"stock\"\n    \n    hc\n    \n  })\n  \n  output$hc2 <- renderHighchart({\n    hc2 <- comment_sent %>% \n      filter (Vendor == vendor2()) %>% \n      hchart(\"line\", hcaes(x=as.Date(`as.Date(Comment_created_time)`), y = sentiment)) %>%        \n      hc_xAxis(type = \"datetime\") %>%\n      hc_title (text = sprintf(\"Comments' sentiments of %s\", toupper(vendor2())))\n    hc2 <- hc_tooltip(hc2, pointFormat = \"Daily Sentiment Score: {point.y}\")\n    \n    hc2$x$type <- \"stock\"                              #\"stock\"\n    \n    hc2\n    \n  })\n```\n\n\n#Conclusion\n\nWell, this was just a toy, but still it reveals some interesting things:\n\n- First, telco vendors messages are more diverged than I expected. It looks that some are more willing to explore \"non traditional\" telco areas than others. \n- I found a lot of similarities between Ericsson and ZTE key themes in all quarters. \n- Even those who are not familiar with the telco business can conclude that the two major events of the industry are CES and MWC.\n- There is quite a big dissimilarity between the traditional telco vendor' messages and the others. Though telco and IT converge, it looks that telco vendors are more \"traditional\" than the benchmarked other players.\n- I had hard time concluding anything from the comment sentiment score graphs. Maybe as next step I will correlate the scores and messages of the same dates to see what caused extra peaks and drops. \n\n#Annex\nGithub repo: https://github.com/imreboda/telco_text\n",
    "created" : 1522478376903.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3044265538",
    "id" : "C7F07832",
    "lastKnownWriteTime" : 1522586841,
    "last_content_update" : 1522586841,
    "path" : "C:/bd/bd_DataCEU/Blog/content/post/CEU.Rmd",
    "project_path" : null,
    "properties" : {
        "ignored_words" : "Arino,de,Rubia,Telco,telcos,http,boda,ceudata,tf,idf,Rstudio,RStudio,https,Rfacebook,fboAuth,www,adweek,tv,watson,AllPostsComments,pre,tokenize,stopwords,zte's,tokenized,redis,bing,Silge,selectbox,selectboxes,telco,wordcloud,ui,selectable,vendorselect,selectInput,jitter,wordclouds,inputSelect,other's,ZTE's,highcharts,highchart,benchmarked,Github,repo,github,imreboda\n"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}