<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.37.1" />


<title>CEU Unstructured Text Analysis Seminar - Assignment - Imre Boda blog</title>
<meta property="og:title" content="CEU Unstructured Text Analysis Seminar - Assignment - Imre Boda blog">



  






<link rel="stylesheet" href="../../../../css/main.css" media="all">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400|Lato:400,400italic,700">

  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png" 
         width="50" 
         height="50" 
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">About</a></li>
    
    <li><a href="https://github.com/imreboda/">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">19 min read</span>
    

    <h1 class="article-title">CEU Unstructured Text Analysis Seminar - Assignment</h1>

    
    <span class="article-date">March 30, 2018</span>
    

    <div class="article-content">
      <div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This is my term assignment of the Unstructured Text Analysis Seminar at CEU in the 2018 winter semester. The course was held by Mr. Eduardo Arino de la Rubia. The assignment was about to create something that use the stuff taught and learnt in the course. I used a lot from the techniques described in the great book “Text Mining with R” by Julia Silge and David Robinson.</p>
<p>My work is a Facebook post and comment analyzer shiny dashboard. It looks at the Facebook pages of a few telco and related vendors (to some extent related to telcos’ traditional or new business segments), and presents / compares their key marketing themes during the past few quarters. Also, looks at the comments to the posts and does a sentiment analysis on the them along the time. At the time of writing this blog, the program is hosted at AWS, available at <a href="http://boda.ceudata.net:3838/" class="uri">http://boda.ceudata.net:3838/</a>.</p>
<p>The dashboard consists of 3 main parts:</p>
<ul>
<li>Presenting the key post themes of the selected vendor per quarter,</li>
<li>Compares the key Facebook themes of the selected vendors to a benchmark vendor,</li>
<li>Shows the sentiment scores of the two vendors along time.</li>
</ul>
<p>Key Facebook “themes” in this context are the most often used phrases (words and bi-grams). When I played with more sophisticated algorithms (tf-idf, LDA), they showed no better result. Maybe this is due to the nature of these posts, so finally I stayed with this simple way.</p>
<p>Sentiment scoring is based on summing up the words with positive / negative sentiment load, correcting it with a bi-gram sentiment check (making sure that terms such as “not nice” are properly calculated as negative not as positive).</p>
<div class="figure">
<img src="../../../../img/Dashboard.png" alt="Dashboard screenshot." />
<p class="caption">Dashboard screenshot.</p>
</div>
<p>The main steps of the works were the followings:</p>
<ul>
<li>Setting up the infrastructure on AWS,</li>
<li>Obtaining Facebook token,</li>
<li>Writing a script that pulls, stores and analyses posts &amp; comments from Facebook,</li>
<li>Scheduling daily execution in Jenkins,</li>
<li>Creating the shiny app.</li>
</ul>
</div>
<div id="setting-up-the-infrastructure-on-aws" class="section level1">
<h1>Setting up the infrastructure on AWS</h1>
<p>It consisted of the following steps:</p>
<ul>
<li>Creating EC2 instance with Ubuntu,</li>
<li>In the “Security Group” settings opening ports for Rstudio, Jenkins and Shiny,</li>
<li>Installing RStudio, creating RStudio admin user on Ubuntu</li>
<li>Installing shiny and shiny server,</li>
<li>Installing R packages from the command line in order to make them available for all R users (needed for script scheduled from Jenkins),</li>
<li>Installing Jenkins, customizing, creating Jenkins admin user.</li>
</ul>
</div>
<div id="obtaining-a-facebook-token" class="section level1">
<h1>Obtaining a Facebook token</h1>
<p>Fairly short, but for a Facebook analysis obviously inevitable workflow:</p>
<ul>
<li>Registering at Facebook Developer page (<a href="https://developers.Facebook.com" class="uri">https://developers.Facebook.com</a>),</li>
<li>Creating a new App, getting the App ID and App Secret,</li>
<li>On Facebook App login setting site URL of the App to “<a href="http://localhost:1410/" class="uri">http://localhost:1410/</a>”,</li>
<li>From Rstudio (with Rfacebook installed), starting fboAuth with the App ID and App Secret parameters obtained in the previous step,</li>
<li>Following the instructions seen on the console,</li>
<li>Saving the received token somewhere for later use.</li>
</ul>
</div>
<div id="the-script-that-pulls-stores-post-and-comment-data-from-facebook" class="section level1">
<h1>The script that pulls / stores post and comment data from Facebook</h1>
<p>This script is run daily, as scheduled from Jenkins. It fetches and stores Facebook posts and comments of selected vendors. It every night scrapes all such data for a given period, regardless that all except last day’s info have been collected before. I thought that disregarding earlier collected data and recollecting again is a cleaner (though not economic) way then relaying on data gathered and stored before, checking if that data is fully valid and not corrupted in any way, no new comments received to earlier posts, etc. Obtaining (dominantly the same) data every day is more work for machines, but this is why we have machines. It is cleaner, and after all, the data volume is not super high, so it causes no disturbance to anyone.</p>
<div id="fetching-a-bit-cleaning-and-transforming-data-from-facebook" class="section level3">
<h3>Fetching, a bit cleaning and transforming data from Facebook</h3>
<p>First, it loads the Facebook Authentication token (saved as per the previous chapter).</p>
<p>Then in the main section from a given date (At the time of writing it is “2017-01-01”. I don’t think I will leave this running for long, but if I will, then it will be modified to be 5 - 7 quarters back from the actual date) until the day before, in a cycle it does the following:</p>
<ul>
<li>For each vendor’s page it reads the posts in 5 days chunks, as the number of posts returned in one query has a limit by Facebook. Transforms the date info a bit, as some display will be done on a per quarter basis, and stores the result in a “posts table”.</li>
<li>For each collected post, it reads all comments and stores them in another table, the “comments table”.</li>
<li>Since many posts do not have message field, but they point to an article link with meaningful URL (e.g. “<a href="http://www.adweek.com/tv-video/how-watson-is-digging-through-decades-of-video-to-help-find-new-sources-of-revenue/" class="uri">http://www.adweek.com/tv-video/how-watson-is-digging-through-decades-of-video-to-help-find-new-sources-of-revenue/</a>”), the script extracts meaningful info from the URL for such posts with empty message field. It searches for patterns that starts with “/”, followed by at least 3 times “one or more word characters followed by -” then again some word characters (such pattern is “how-watson-is-digging-through-decades-of-video-to-help-find-new-sources-of-revenue/”). The reason for requesting at least 3 times of “word characters plus ‘-’” sequences and then some word characters is to exclude patterns like “/2017-03-01/”, pretty typical in URLs. Such patterns are cleaned from leading “/” and separator “-” characters and copied to a new field called “Headline”, to make up for the empty “message” field. In vast majority of the cases this seems to do the job.</li>
</ul>
<pre class="r"><code>### extracting headline from news link, if message is empty
VendorPosts_womessage &lt;- VendorPosts%&gt;%
    filter(is.na(message)) %&gt;%
    mutate (Headline = str_extract (link, &quot;/(\\w+-){3,}[\\w]+&quot;)) %&gt;%    #search for patterns like &quot;/5g-connected-trials-c..&quot;
    mutate (Headline = str_replace (Headline, &quot;^/&quot;,&quot;&quot;)) %&gt;%   #strip leading / 
    mutate (Headline = str_replace_all (Headline, &quot;-&quot;, &quot; &quot;)) </code></pre>
<ul>
<li>For the rest of the posts, which do contain “message” info, this field is copied to the newly created “Headline” column, getting rid of potential “:http…” references, which often appended to the end of Facebook posts in the form of “read more at: http…”, “learn more: http…” and alike.</li>
</ul>
<pre class="r"><code>### copy message to headline for the rest
  VendorPosts_wmessage &lt;- VendorPosts %&gt;% 
    filter(!is.na(message)) %&gt;%
    mutate (Headline = message) %&gt;%
    mutate (Headline = str_replace (Headline, &quot;htt.*$&quot;, &quot;&quot;)) %&gt;%    #remove http.....
    mutate (Headline = str_replace (Headline, &quot;:\\s*$&quot;, &quot;&quot;))        #remove &quot;: &quot;, mostly inherited from &quot;: http...&quot;</code></pre>
<p>This way all real or created messages are placed in column “Headline”, (most often) containing the story.</p>
<ul>
<li>As mentioned above, messages often contain “learn more”, “can find”,… phrases, which are in this context to be treated as junks, so these are filtered out.</li>
</ul>
<pre class="r"><code>  VendorPosts &lt;- VendorPosts %&gt;%
    mutate (Headline = str_replace_all( Headline, &quot;read more&quot;, &quot;&quot;)) %&gt;%
    mutate (Headline = str_replace_all (Headline, &quot;learn more&quot;, &quot;&quot;)) %&gt;%
    mutate (Headline = str_replace_all (Headline, &quot;can find&quot;, &quot;&quot;))</code></pre>
<ul>
<li>Finally, a mega table creating all posts and relevant comments are created with joining “posts table” and “comments table”. The above are repeated for all vendors, and the result (posts and comments) are stored in “AllPostsComments”.</li>
</ul>
<pre class="r"><code>VendorPosts &lt;- VendorPosts %&gt;% left_join(VendorCommentsTable, by = c(&quot;id&quot;=&quot;PostID&quot;))

  ### Putting to one list  
AllPostsComments &lt;- rbind (AllPostsComments, VendorPosts)</code></pre>
<p>The final table is saved in a file, just in any case.</p>
</div>
<div id="fetching-a-bit-cleaning-and-transforming-data-from-facebook-1" class="section level2">
<h2>Fetching, a bit cleaning and transforming data from Facebook</h2>
<p>Once all above are done, comes the shallow text mining part, which consists of two parts:</p>
<ul>
<li>Finding the key post themes,</li>
<li>Sentiment analysis on the comments.</li>
</ul>
<div id="finding-key-themes-in-posts" class="section level4">
<h4>Finding key themes in posts</h4>
<p>As said above, this is based on simply getting the most frequent words and bi-grams in the posts. As mentioned in the introduction section, I played with tf-idf and LDA, but their output was not better than the simple term frequency, so that was selected for the finally implemented version.</p>
<p>As themes are presented and compared on a per vendor and per quarter basis, 5 most frequent words and 5 most frequent bi-grams are to be marked per quarter per vendor. It is done in two rounds, first for uni-grams and then for bi-grams. The reason for doing it in two separate rounds is that naturally word frequencies are higher than bi-gram frequencies, therefor if top phrases would be collected in one go, only words could qualify for the top 5 or top 10 places, not bi-grams.</p>
<p>First we get rid of duplicate posts (introduced when we merged “posts table” and “comments table”, as one post had typically several comments), word tokenize the “Headline” column, get rid of normal stopwords and special stopwords (containing the vendors’ names and some words that they frequently use in posts so here become junk).</p>
<p>Then the words’ frequency per vendor per quarter is added to each remaining word.</p>
<pre class="r"><code>tidyPosts &lt;- AllPostsComments %&gt;%
  group_by(id) %&gt;%                #from here until ungroup: get rid of duplicate posts
  filter(row_number() == 1) %&gt;%
  ungroup %&gt;%
  unnest_tokens (word, Headline) %&gt;%
  filter(!is.na(word)) %&gt;%
  anti_join (stp_en) %&gt;%
  anti_join (stp_spc) %&gt;%
  mutate(word = str_replace(word, &quot;\\w+[^\\w]\\w*&quot;, &quot;&quot;)) %&gt;%   #&quot;zte&#39;s&quot; was not propoerly filtered by stp_spc
  filter(word != &quot;&quot;) 

proportion &lt;- tidyPosts %&gt;%
  count (Vendor, created_Q, word, sort = TRUE) %&gt;%   #count words per vendor per Q
  group_by (Vendor, created_Q) %&gt;%
  mutate (prop =  n/sum(n) * 100) %&gt;%
  arrange (desc(prop)) %&gt;%
  arrange (Vendor, created_Q)</code></pre>
<p>Bi-grams are treated pretty similarly, except that for getting rid of stopwords, “specific” stopwords and “zte’s” like words, bi-grams are temporarily separated, and after the stopwords removal united back.</p>
<pre class="r"><code>bigramPosts &lt;- AllPostsComments %&gt;%
  group_by(id) %&gt;%                #from here until ungroup: get rid of duplicate posts
  filter(row_number() == 1) %&gt;%
  ungroup %&gt;%
  unnest_tokens (bigram, Headline, token = &quot;ngrams&quot;, n = 2) %&gt;%
  filter(!is.na(bigram)) %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;%   #storing bigram words separately
  filter(!word1 %in% stp_en$word) %&gt;%
  filter(!word2 %in% stp_en$word) %&gt;%
  filter(!word1 %in% stp_spc$word) %&gt;%
  filter(!word2 %in% stp_spc$word) %&gt;%
  mutate(word1 = str_replace(word1, &quot;\\w+[^\\w]\\w*&quot;, &quot;&quot;)) %&gt;%   #&quot;zte&#39;s&quot; was not propoerly filtered by stp_spc
  filter(word1 != &quot;&quot;)  %&gt;%
  mutate(word2 = str_replace(word2, &quot;\\w+[^\\w]\\w*&quot;, &quot;&quot;)) %&gt;%   #&quot;zte&#39;s&quot; was not propoerly filtered by stp_spc
  filter(word2 != &quot;&quot;)  %&gt;%
  unite(word, word1, word2, sep = &quot; &quot;) 

proportion_bi &lt;- bigramPosts %&gt;%   #same as for unigrams, not collapsed as these have different weights
  count (Vendor, created_Q, word, sort = TRUE) %&gt;%   #count words per vendor per month
  group_by (Vendor, created_Q) %&gt;%
  mutate (prop =  n/sum(n) * 100) %&gt;%
  arrange (desc(prop)) %&gt;%
  arrange (Vendor, created_Q) </code></pre>
<p>Finishing the key phrase selection parts, the top 5 words and bi-grams (per vendor per quarter) are put in one list and saved. The same way the lists of all (valid) words and bi-grams with their frequency proportion are saved.</p>
<pre class="r"><code>prop_top5 &lt;- rbind (
  proportion  %&gt;%
    filter (row_number () &lt;6L),
  proportion_bi  %&gt;%
    filter (row_number () &lt;6L)
)
fwrite (prop_top5, paste0(DIR, &quot;/&quot;, &quot;prop_top5.csv&quot;))


proportion &lt;- rbind (proportion, proportion_bi)
fwrite (proportion, paste0(DIR, &quot;/&quot;, &quot;proportion.csv&quot;))</code></pre>
<p>These files are used to pass information between Facebook input collector and shiny display output functions. For a while I went for an alternative approach: to use redis as a postbox. I ended up with the file based version as files are useful anyhow to cope with shutdown and similar situations. And as such reads do not take place frequently (at least as long as the dashboard does not attract millions of visitors :) ), it should be fine.</p>
</div>
<div id="comments-sentiment-analysis" class="section level4">
<h4>Comments sentiment analysis</h4>
<p>Concluding the text mining parts, the sentiments of comments to vendors’ post are analysed based on the “bing” sentiment lexicon. The idea is that words with positive and negative sentiments are counted for every day for every vendor, and the difference of the two will be the sentiment score of the vendor for that day.</p>
<p>The above idea is slightly enhanced here, as bi-grams are also looked at, and if there is a negating word (e.g “not”, “isn’t”, etc.) preceding an adjective word, then the sentiment classification of the word is negated.</p>
<p>Agree, this is still not “bullet proof” as there are negations that are still not treated (and not only sarcastic sentences or complex structures, but simple cases such as “not quite good”, not to mention comments written in non-English language), but probably it is a safe assumption that these cases are just minority, and the result is “acceptably good enough”.</p>
<p>So as before, the analysis is run in two steps.</p>
<p>First, words with sentiment load are counted per vendor and per day.</p>
<pre class="r"><code>bing &lt;- (get_sentiments( &quot;bing&quot;))


tidy_word_Comments &lt;- AllPostsComments %&gt;%
  unnest_tokens (word, Comment_message) %&gt;%
  filter(!is.na(word)) %&gt;%
  filter(word != &quot;&quot;) 

uni_res &lt;- tidy_word_Comments %&gt;%
  inner_join (bing) %&gt;%
  count (Vendor, as.Date(Comment_created_time), sentiment) %&gt;%     #Comment_created_wk for weekly display version[]
  spread (sentiment, n, fill = 0) %&gt;%
  mutate (sentiment = positive - negative) </code></pre>
<p>Then for correcting the cases when an adjective is preceded with a negation word, first such cases are filtered. Then the sentiment score of the bi-gram based on its <strong>second word</strong> is counted for every day (and every vendor), and then negated.</p>
<p>Finally, the original sentiment scores for every day are corrected with the above “delta sentiment” scores by joining the result of the two calculations and subtracting twice the correction scores (the twice has to be calculated, as e.g. for “not good” in the uni-gram score we increased the sentiment, adding the negated score once would only take it to neutral, another correction is needed to get the correct score).</p>
<p>As always, the result is written to a file to pass it to the dashboard application.</p>
<pre class="r"><code>## bi-grams to take care of negations
negation_words &lt;- c(&quot;not&quot;, &quot;no&quot;, &quot;never&quot;, &quot;without&quot;, &quot;isn&#39;t&quot;, &quot;aren&#39;t&quot;)


tidy_bigram_Comments &lt;- AllPostsComments %&gt;%
  unnest_tokens (bigram, Comment_message, token = &quot;ngrams&quot;, n = 2) %&gt;%
  filter(!is.na(bigram)) %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)    #storing bigram words separately

negated_words_res &lt;- tidy_bigram_Comments %&gt;%
  filter(word1 %in% negation_words) %&gt;%
  inner_join(bing, by = c(&quot;word2&quot; = &quot;word&quot;)) %&gt;%
  count (Vendor, as.Date(Comment_created_time), sentiment) %&gt;%   
  mutate(sentiment = recode (sentiment, &quot;positive&quot; = &quot;negative&quot;, &quot;negative&quot; = &quot;positive&quot;)) %&gt;%    #invert
  spread (sentiment, n, fill = 0) %&gt;%
  mutate (delta_sentiment = positive - negative) %&gt;%
  select (-c(negative, positive))

#joining the result of bigram and calculate sentiment
comment_sent &lt;- left_join(uni_res, negated_words_res) %&gt;%
  mutate (delta_sentiment = ifelse(is.na(delta_sentiment),0,delta_sentiment)) %&gt;%
  mutate (sentiment = sentiment + 2 * delta_sentiment)</code></pre>
</div>
</div>
</div>
<div id="scheduling-the-facebook-post-and-comment-collector-and-analyzer" class="section level1">
<h1>Scheduling the Facebook post and comment collector and analyzer</h1>
<p>The above script is schedule in Jenkins to run daily as shown in the below screenshot.</p>
<div class="figure">
<img src="../../../../img/JenkinsSchedule.png" alt="Jenkins scheduler setting" />
<p class="caption">Jenkins scheduler setting</p>
</div>
</div>
<div id="the-shiny-dashboard-app" class="section level1">
<h1>The shiny dashboard app</h1>
<p>The standard shiny Dashboard layout is used, which consists of a header, a sidebar and a main body part.</p>
<p>The sidebar contains nothing else but a selectbox to choose the vendor to be analyzed.</p>
<p>The body section consists of three parts:</p>
<ul>
<li>The upper part displays the key themes of the selected vendor by quarters. The most often used 5 “real” words and bi-grams are regarded as key topics, as described above.</li>
<li>The middle layer allows a quarter and a second vendor selection via two other selectboxes. The idea is to allow some comparison to reveal how similar or different two telco vendors’ key themes are; to see if there are any significant differences between their slogans that might reveal different market strategy. So under the selectboxes the two vendors’ key themes in the selected quarter are plotted (in wordcloud), and also a graph that displays the frequencies of the themes (words and bi-grams) used by both vendors. This graph reveals how many common themes these have in their posts (the more words presented on the plot), and also how much emphasis the vendors give to these themes (the closer the points are to the diagonal line, the more similar emphasis is given to a theme represented by the word/bi-gram),</li>
<li>The bottom part shows the sentiment score of the comments given to the vendors’ posts by time.</li>
</ul>
<p>The dashboard is written in a single app file, containing both the ui and the server functions.</p>
<div id="the-input-selectors" class="section level4">
<h4>The input selectors</h4>
<p>There are three selectbox inputs.</p>
<p>The first one allows to chose the “main” vendor. This is looked at in all three dashboard sections.</p>
<p>Its selectbox is placed in the sidebar as shown below:</p>
<pre class="r"><code>sidebar &lt;- dashboardSidebar(
  
  selectInput(inputId = &quot;vendorselect&quot;,
              label = &quot;Select Vendor&quot;,
              choices = Vendor_list)
  )</code></pre>
<p>Vendor_list is read at the server start (in case of change in vendors, it has to be restarted).</p>
<p>The second selector allows a “comparison” vendor to select, this is the one whose market themes are compared to the “main” vendor’s in the middle section, and the comments’ sentiments also in the bottom section.</p>
<p>Giving value to this is slightly more work than doing for the “main” vendor.</p>
<p>First, because here we need to make sure not to select the “main” vendor; the latter must be excluded from the list of selectable vendors.</p>
<p>Second, because simply excluding the “main” vendor from the selectable list via “choices = Vendor_list [Vendor_list != input$vendorselect]” results in an error message displayed at the middle section’s middle plot for about half a second. After that it disappears, so not a big deal indeed, but still not nice. In order to get rid of this disturbing error, a reactive variable is introduced, which is first initialized with the second item on the vendors’ list. When the user selects a second vendor, this reactive variable is getting new value. All plotting functions presenting anything about second vendor will use this reactive variable, and not directly the output if the input selector for the second vendor. This way these plotting functions will always get non-NULL value.</p>
<pre class="r"><code>  vendor2 &lt;- reactiveVal(Vendor_list [2]) 
  
  output$select_vendor2 &lt;- renderUI({
    selectInput(inputId = &quot;vendor2&quot;,
                label = &quot;Select Another Vendor to compare&quot;,
                choices = Vendor_list [Vendor_list != input$vendorselect])
  })
  
  observeEvent(input$vendor2, {
    vendor2(input$vendor2)             # rv$value &lt;- newValue
  })</code></pre>
<p>The third input selector allows picking the quarter for the theme comparison in the middle section. It is a simple selectInput, the same as in the sidebar, except that here the list to be selected from is the list of quarters of the analysis period.</p>
</div>
<div id="the-output-functions-plots" class="section level4">
<h4>The output functions: plots</h4>
<div id="top-section" class="section level5">
<h5>Top section</h5>
<p>It is a simple “jitter plot” with the relative frequency of the word / bi-gram on the y, and quarter on the x axis. The input is the top5 (top5 per word plus top5 per bi-gram, per quarter per vendor) results of the post and comment reader and processor script.</p>
<pre class="r"><code>output$plot1 &lt;- renderPlot({prop_top5 %&gt;% filter (Vendor == input$vendorselect)%&gt;%
      ggplot (aes(created_Q, prop, color=created_Q, label = word)) + geom_jitter (size =0) + 
      geom_text_repel (size=5, segment.color = &quot;white&quot;) + theme_bw (base_size = 24) + 
      theme(legend.position=&quot;none&quot;, panel.grid.major=element_blank(),
            panel.grid.minor=element_blank(), axis.text.y=element_blank(), axis.line = element_blank()) +
      labs(title = sprintf (&quot;Most frequent themes in %s Facebook posts&quot;, toupper(input$vendorselect))) +
           xlab(&quot;Quarters&quot;) + ylab  (&quot;Frequency&quot;)
    })</code></pre>
</div>
<div id="middle-section" class="section level5">
<h5>Middle section</h5>
<p>The two wordclouds at the two ends work pretty much the same way, except that the one for the “comparison” vendor uses the reactive variable, while the other one can take the result of its inputSelect directly. They both work on the same data as the plot in the top section.</p>
<pre class="r"><code>  output$cloud1 &lt;- renderPlot({
    prop_top5 %&gt;% filter (Vendor == input$vendorselect) %&gt;% filter (created_Q == input$quarterselect) %&gt;%
      with(wordcloud(word, n, max.words = 100, min.freq = 0.2, scale = c(3,0.5), colors = c(&quot;#0091ff&quot;, &quot;#f0650e&quot;)))  
  })
  output$cloud2 &lt;- renderPlot({
    prop_top5 %&gt;% filter (Vendor == vendor2()) %&gt;% filter (created_Q == input$quarterselect) %&gt;% 
      with(wordcloud(word, n, max.words = 100, min.freq = 0.2, scale = c(3,0.5),colors = c(&quot;#0091ff&quot;, &quot;#f0650e&quot;)))  
  })</code></pre>
<p>The middle plot looks at the frequency table of all uni-grams and bi-grams and plots it on a “jitter plot”. For every word coming to the plot the relative frequency in the “comparison” vendor’s frequency table is presented on the x axis, while on the y axis the same for the “main” vendor.</p>
<p>The scale is logarithmic, consequently, words and bi-grams that are present in one but not in the other’s posts are not even shown. This means that the less points are on the displays, the less common themes the vendors have. And as said above, the closer is a point to the diagonal line, it has more similar frequency in the vendors’ posts. In an oversimple interpretation: getting the same focus.</p>
<pre class="r"><code>  output$CompPlot &lt;- renderPlot ({
    proportion %&gt;%
      filter (created_Q == input$quarterselect) %&gt;%
      filter (Vendor %in% c(input$vendorselect, vendor2())) %&gt;%
      select (-n) %&gt;%     #drop n, in order  to allow spread and collapse (gather)
      spread (Vendor, prop) %&gt;%    #spred prop per vendor
      gather (Vendor, prop, -created_Q, -word, -c(input$vendorselect, vendor2())) %&gt;%
      ggplot ( aes(get(vendor2())/100, get(input$vendorselect)/100, 
                   color = abs (get(input$vendorselect)-get(vendor2())))) + 
      geom_abline (color = &quot;gray40&quot;, lty =2) + 
      geom_jitter (alpha = 0.3, size = 2.5, width = 0.3, height = 0.3) +
      geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
      scale_x_log10(labels = percent_format(), limits = c(0.001,0.1)) +
      scale_y_log10(labels = percent_format(), limits = c(0.001,0.1)) +
      scale_color_gradient(limits = c(0, 2), low = &quot;#0091ff&quot;, high = &quot;#f0650e&quot;) +
      #facet_wrap (~ Vendor, ncol = 5) +
      theme_bw() + theme(legend.position=&quot;none&quot;) +
      labs(y = input$vendorselect, x = vendor2())
      
  })</code></pre>
<p>At the beginning I planned to represent such similarity and dissimilarity with commonality and comparison clouds from the same “wordcloud” package. However, it seemed not to be trivial how to cope with cases when a message was in high focus at both vendors, but with different weights. (e.g. this was the case with Ericsson and ZTE: both had 5G as their #1 topic by far, but as in Ericsson’s posts it appeared with even higher frequency than in ZTE’s, it was shown as a difference between the two. Whereas the reality is that it was by far the most exposed topic at both vendors, hence, it is a strong similarity in their strategic focus, not dissimilarity).</p>
<p>I am sure that there is some nice way to overcome this, but for the sake of time I decided to leave the cloud comparison for “inside” vendors’ topics and use this (visually less attractive) solution: cloud where font sizes represent frequency among the given vendor’s topics (not across vendors).</p>
</div>
<div id="the-bottom-section" class="section level5">
<h5>The bottom section</h5>
<p>Finally, there come two highcharts to present the comments sentiment scores for the two selected vendors: the upper one displays the “main” vendor’s by time, the lower one the “comparison” vendor’s. I picked the “stock” type highchart for such display, because this creates amazing timeline charts.</p>
<p>The two outputs are pretty much alike, the only notable difference is that the second one uses here again the reactive variable, while the main vendors’ chart takes value from the relevant input selector directly.</p>
<pre class="r"><code>output$hc &lt;- renderHighchart({
    hc &lt;- comment_sent %&gt;% 
      filter (Vendor == input$vendorselect) %&gt;% 
      hchart(&quot;line&quot;, hcaes(x=as.Date(`as.Date(Comment_created_time)`), y = sentiment)) %&gt;%        
      hc_xAxis(type = &quot;datetime&quot;) %&gt;%
      hc_title (text = sprintf(&quot;Comments&#39; sentiments of %s&quot;, toupper(input$vendorselect)))
    hc &lt;- hc_tooltip(hc, pointFormat = &quot;Daily Sentiment Score: {point.y}&quot;)
      
    hc$x$type &lt;- &quot;stock&quot;                              #&quot;stock&quot;
    
    hc
    
  })
  
  output$hc2 &lt;- renderHighchart({
    hc2 &lt;- comment_sent %&gt;% 
      filter (Vendor == vendor2()) %&gt;% 
      hchart(&quot;line&quot;, hcaes(x=as.Date(`as.Date(Comment_created_time)`), y = sentiment)) %&gt;%        
      hc_xAxis(type = &quot;datetime&quot;) %&gt;%
      hc_title (text = sprintf(&quot;Comments&#39; sentiments of %s&quot;, toupper(vendor2())))
    hc2 &lt;- hc_tooltip(hc2, pointFormat = &quot;Daily Sentiment Score: {point.y}&quot;)
    
    hc2$x$type &lt;- &quot;stock&quot;                              #&quot;stock&quot;
    
    hc2
    
  })</code></pre>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Well, this was just a toy, but still it reveals some interesting things:</p>
<ul>
<li>First, telco vendors messages are more diverged than I expected. It looks that some are more willing to explore “non traditional” telco areas than others.</li>
<li>I found a lot of similarities between Ericsson and ZTE key themes in all quarters.</li>
<li>Even those who are not familiar with the telco business can conclude that the two major events of the industry are CES and MWC.</li>
<li>There is quite a big dissimilarity between the traditional telco vendor’ messages and the others. Though telco and IT converge, it looks that telco vendors are more “traditional” than the benchmarked other players.</li>
<li>I had hard time concluding anything from the comment sentiment score graphs. Maybe as next step I will correlate the scores and messages of the same dates to see what caused extra peaks and drops.</li>
</ul>
</div>
<div id="annex" class="section level1">
<h1>Annex</h1>
<p>Github repo: <a href="https://github.com/imreboda/telco_text" class="uri">https://github.com/imreboda/telco_text</a></p>
</div>

    </div>
  </article>

  

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../../../../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../../../../images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>

    
  </body>
</html>

